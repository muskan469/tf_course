
import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np

# Load the SubwordTextEncoder with the correct unk_token
try:
    # Try loading with the .txt extension if that's what the user has
    tokenizer = tfds.deprecated.text.SubwordTextEncoder.load_from_file(
        'imdb_vocab_subwords.txt', 
        unk_token='[UNK]'
    )
except tf.errors.NotFoundError:
    # If the above fails (e.g. because it appends .subwords), try without extension
    # assuming the user might have the file named properly elsewhere or the behavior differs
    try:
        tokenizer = tfds.deprecated.text.SubwordTextEncoder.load_from_file(
            'imdb_vocab_subwords', 
            unk_token='[UNK]'
        )
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        # Fallback: Create from vocab list manually if file loading acts up
        with open('imdb_vocab_subwords.txt', 'r') as f:
            vocab_list = [line.strip() for line in f]
        tokenizer = tfds.deprecated.text.SubwordTextEncoder(
            vocab_list=vocab_list,
            unk_token='[UNK]'
        )

# Test the tokenizer
sample_string = 'TensorFlow, from basics to mastery'
tokenized_string = tokenizer.encode(sample_string)
print(f'Tokenized string is {tokenized_string}')

original_string = tokenizer.decode(tokenized_string)
print(f'The original string: {original_string}')
