{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303f946f",
   "metadata": {},
   "source": [
    "Learning tokenization of word instead of vectorizing the whole word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932da3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3faafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\", as_supervised=True, download=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b536d2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
      "    classification containing substantially more data than previous benchmark\n",
      "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
      "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_dir='/Users/muskan/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    nondeterministic_order=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78167354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 09:08:36.196716: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:396] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2026-02-17 09:08:36.201238: W tensorflow/core/kernels/data/cache_dataset_ops.cc:917] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2026-02-17 09:08:36.201338: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews = imdb['train'].map(lambda review, label: review)\n",
    "train_labels = imdb['train'].map(lambda review, label: label)\n",
    "\n",
    "test_reviews = imdb['test'].map(lambda review, label: review)\n",
    "test_labels = imdb['test'].map(lambda review, label: label)\n",
    "\n",
    "list(train_reviews.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a33d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subword tokenization\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 120\n",
    "PADDING_TYPE = 'pre'\n",
    "TRUNC_TYPE = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b404a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 09:09:07.807047: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "\n",
    "vectorize_layer.adapt(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca5637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_func(sequences):\n",
    "    sequences = sequences.ragged_batch(batch_size=sequences.cardinality())\n",
    "    sequences = sequences.get_single_element()\n",
    "    padded_sequences = tf.keras.utils.pad_sequences(sequences.numpy(),\n",
    "                                                    maxlen = MAX_LENGTH,\n",
    "                                                    padding=PADDING_TYPE,\n",
    "                                                    truncating=TRUNC_TYPE)\n",
    "    padded_sequences = tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
    "\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fbe62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = train_reviews.map(lambda text: vectorize_layer(text)).apply(padding_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510830ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[   0    0    0    0   11   14   34  412  384   18   90   28    1    8\n",
      "   33 1322 3560   42  487    1  191   24   85  152   19   11  217  316\n",
      "   28   65  240  214    8  489   54   65   85  112   96   22 5596   11\n",
      "   93  642  743   11   18    7   34  394 9522  170 2464  408    2   88\n",
      " 1216  137   66  144   51    2    1 7558   66  245   65 2870   16    1\n",
      " 2860    1    1 1426 5050    3   40    1 1579   17 3560   14  158   19\n",
      "    4 1216  891 8040    8    4   18   12   14 4059    5   99  146 1241\n",
      "   10  237  704   12   48   24   93   39   11 7339  152   39 1322    1\n",
      "   50  398   10   96 1155  851  141    9], shape=(120,), dtype=int32)\n",
      "    this was an absolutely terrible movie dont be [UNK] in by christopher walken or michael [UNK] both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the [UNK] rebels were making their cases for [UNK] maria [UNK] [UNK] appeared phony and her [UNK] affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher [UNK] good name i could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "imdb_vocab_fullword = vectorize_layer.get_vocabulary()\n",
    "\n",
    "sample_sequence = train_sequences.take(1).get_single_element()\n",
    "\n",
    "print(sample_sequence)\n",
    "\n",
    "decoded_text = [imdb_vocab_fullword[token] for token in sample_sequence]\n",
    "\n",
    "decoded_text = ' '.join(decoded_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875f5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    train_reviews, \n",
    "    vocabulary_size = 8000,\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    vocabulary_output_file=\"imdb_vocab_subwords.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e072c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary = \"./vocab.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06746543",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "PREFETCH_BUFFER_SIZE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE=32\n",
    "\n",
    "train_sequences_subword = train_reviews.map(lambda review: subword_tokenizer.tokenize(review)).apply(padding_func)\n",
    "test_sequences_subword = test_reviews.map(lambda review: subword_tokenizer.tokenize(review)).apply(padding_func)\n",
    "\n",
    "train_dataset_vectorized = tf.data.Dataset.zip(train_sequences_subword,train_labels)\n",
    "test_dataset_vectorized = tf.data.Dataset.zip(test_sequences_subword,test_labels)\n",
    "\n",
    "# Optimize the datasets for training\n",
    "train_dataset_final = (train_dataset_vectorized\n",
    "                       .shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "                       .cache()\n",
    "                       .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                       .batch(BATCH_SIZE)\n",
    "                       )\n",
    "\n",
    "test_dataset_final = (test_dataset_vectorized\n",
    "                      .cache()\n",
    "                      .prefetch(buffer_size=PREFETCH_BUFFER_SIZE)\n",
    "                      .batch(BATCH_SIZE)\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1275d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensionality of the embedding\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(MAX_LENGTH,)),\n",
    "    tf.keras.layers.Embedding(subword_tokenizer.vocabulary_size(), EMBEDDING_DIM),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42149dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Set the training parameters\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Start training\n",
    "history = model.fit(train_dataset_final, epochs=num_epochs, validation_data=test_dataset_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "  '''Plots the training and validation loss and accuracy from a history object'''\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  epochs = range(len(acc))\n",
    "\n",
    "  fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "  ax[0].plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "  ax[0].plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "  ax[0].set_title('Training and validation accuracy')\n",
    "  ax[0].set_xlabel('epochs')\n",
    "  ax[0].set_ylabel('accuracy')\n",
    "  ax[0].legend()\n",
    "\n",
    "  ax[1].plot(epochs, loss, 'bo', label='Training Loss')\n",
    "  ax[1].plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "  ax[1].set_title('Training and validation loss')\n",
    "  ax[1].set_xlabel('epochs')\n",
    "  ax[1].set_ylabel('loss')\n",
    "  ax[1].legend()\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "plot_loss_acc(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
